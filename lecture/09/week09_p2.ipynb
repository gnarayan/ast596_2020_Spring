{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 09, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Gaussian Processes contd.\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tuesday - We blazed through a worked example of GPs largely to help with HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's step back and go over GPs a bit more slowly.\n",
    "\n",
    "### Recap:\n",
    "\n",
    "The first half of the semester, you got famililar with observations drawn from a distribution e.g.\n",
    "\n",
    "$y1$, drawn from a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_1 | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[ - \\frac{(y_1-\\mu)^2}{2 \\sigma^2} \\right] \n",
    "\\end{align}\n",
    "\n",
    "i.e.\n",
    "\n",
    "### $$y_1 \\sim \\mathcal{N}(\\mu,\\sigma^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If pair of variables $y_1$ and $y_2$, drawn from a *bivariate Gaussian distribution*. The *joint probability density* for $y_1$ and $y_2$ is:\n",
    "\n",
    "### $$\n",
    "\\left[ \\begin{array}{l} y_1 \\\\ y_2 \\end{array} \\right] \\sim \\mathcal{N} \\left(\n",
    "\\left[ \\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}  \\right] , \n",
    "\\left[ \\begin{array}{ll} \n",
    "\\sigma_1^2 & C \\\\\n",
    "C & \\sigma_2^2 \n",
    "\\end{array}  \\right] \n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "### $$C = {\\rm cov}(y_1,y_2)$$ \n",
    "\n",
    "is the *covariance* between $y_1$ and $y_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the first half of the semester, we dealt with independent variables i.e.\n",
    "\n",
    "### $$P(y_1 \\cap y_2) = P(y_1) \\cdot P(y_2) $$\n",
    "\n",
    "and consequently\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "P(y_2|y_1) = \\frac{P(y_1 \\cap y_2)}{P(y_1)} = P(y_2)\n",
    "\\end{align}\n",
    "\n",
    "If two variables are independent, then $C = 0$ (remember converse isn't true). \n",
    "\n",
    "The observations are *uncorrelated* so measuring $y_1$ doesn't teach us anything about $y_2$.\n",
    "\n",
    "(If in addition $\\mu_1 = \\mu_2$ and $\\sigma_1 = \\sigma_2$ the variables are i.i.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# With time-series, $C \\ne 0$ \n",
    "\n",
    "If we know the value of $y_1$, the probability density for $y_2$ collapses to the the *conditional distribution* of $y_2$ given $y_1$:\n",
    "\n",
    "### $$\n",
    "p(y_2 \\mid y_1) = \\mathcal{N} \\left( \\mu_2 + C (y_1-\\mu_1)/\\sigma_1^2, \\sigma_2^2-C^2\\sigma_1^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now consider $N$ variables drawn from a multivariate Gaussian distribution:\n",
    "\n",
    "### $$\n",
    "\\boldsymbol{y} \\sim \\mathcal{N} (\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "### $$\\boldsymbol{y} = (y_1,y_2,\\ldots,y_N)^T$$\n",
    "\n",
    "### $$\\boldsymbol{\\mu} = (\\mu_1,\\mu_2,\\ldots,\\mu_N)^T$$ \n",
    "\n",
    "\n",
    "is the *mean vector*, and $\\boldsymbol{\\Sigma}$ is an $N \\times N$ positive semi-definite *covariance matrix*, with elements \n",
    "\n",
    "### $$\\Sigma_{ij}={\\rm cov}(y_i,y_j)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### And then the likelihood generalizes from 1D:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_1 | \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left[ - \\frac{(y_1-\\mu)^2}{2 \\sigma^2} \\right] \n",
    "\\end{align}\n",
    "\n",
    "### to ND:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "p(\\boldsymbol{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2 \\pi^N |\\Sigma|} } \\exp \\left[ -\\frac{1}{2} (\\boldsymbol{y} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{y} - \\boldsymbol{\\mu}) \\right] \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This works because:\n",
    "\n",
    "<img src=\"gaussians_all_the_way_down.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# A Gaussian process is an extension of this concept to infinite $N$.\n",
    "\n",
    "# This gives rise to a probability distribution over functions, rather than finite $N$ samples. \n",
    "\n",
    "<img src=\"gp.png\">\n",
    "\n",
    "# Informally - infinitely long vector ~ function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Again, for finite number of $y$ drawn from a multivariate normal distribution:\n",
    "\n",
    "### $$\n",
    "\\boldsymbol{y} \\sim \\mathcal{N} (\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\n",
    "$$\n",
    "\n",
    "This clearly doesn't make sense for infinite $N$, but the essential feature remains the same:\n",
    "### A Gaussian process is completely specified by its *mean function* and *covariance function*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incorporating observational error is similar to what you did in the past as well:\n",
    "\n",
    "### $$ y \\sim f(t) + \\epsilon$$ \n",
    "\n",
    "### with deviations from the truth related to the observational uncertainties\n",
    "\n",
    "### $$ \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2) $$\n",
    "\n",
    "\n",
    "### except now, $f(t)$ is a function not of some parameters, but rather of functions thenselves:\n",
    "\n",
    "###  $$ f(t) \\sim \\mathcal{GP}(m(t), k(t,t'))$$\n",
    "\n",
    "### where I'm switching from $\\mu$ to $m(t)$ and $\\Sigma$ to $k(t, t')$ just to make explicit that these are not vectors.\n",
    "\n",
    "I'm using $k$ because this function that describes the covariance between time $t$ and $t'$ is called a **kernel** function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Graphical Model for a GP\n",
    "\n",
    "\n",
    "### Recall:\n",
    "\n",
    "A **probabilistic graphical model** (PGM) is a very useful way of visualizing a generative model.\n",
    "* They sketch out the procedure for how one would generate mock data in practice.\n",
    "* They illustrate the interdependence of model parameters, and the dependence of data on parameters.\n",
    "* _They also (therefore) represent a conditional factorization of the PDF for all the data and model parameters._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ingredients of a PGM:\n",
    "* **Nodes** represent PDFs for parameters\n",
    "* **Edges** represent conditional relationships\n",
    "* **Plates** represent repeated model components whose contents are **conditionally independent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Types of nodes:\n",
    "* **Circles** represent a PDF. This parameter is a *stochastic* function of the parameters feeding into it.\n",
    "* **Points** represent a delta-function PDF. This parameter is a *deterministic* function of the parameters feeding into it.\n",
    "* **Double circles** (or shading) indicate measured data. They are stochastic in the context of generating mock data, but fixed in the context of parameter inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If we were dealing with i.i.d. data\n",
    "\n",
    "### $$ y \\sim f(t) + \\epsilon$$ \n",
    "\n",
    "### with deviations from the truth related to the observational uncertainties\n",
    "\n",
    "### $$ \\epsilon \\sim \\mathcal{N}(0, \\sigma_y^2) $$\n",
    "\n",
    "\n",
    "<img src=\"pgm_conditionally_independent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# With time-series the data are not conditionally independent\n",
    "\n",
    "i.e. you don't have a nice plate:\n",
    "\n",
    "<img src=\"gp_pgm.png\">\n",
    "\n",
    "From [Rasmussen & Williams (aka the GP bible)](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We don't actually observe the function  $f$\n",
    "\n",
    "### As you've seen, there isn't one single function, but infinitely many for a specific choice of $m$, $k$\n",
    "\n",
    "### We marginalize over them to find the posterior mean - $f$ is behaving like a parameter \n",
    "\n",
    "### The paramters of $m$ and $k$, which actually specify $f$ are called \"hyper parameters\"\n",
    "\n",
    "### The interesting bit here is the covariance function/kernel, $k$ (we can always recenter the data to have mean = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Thankfully, in the real world, we only have a finite number of observations\n",
    "\n",
    "Previously, we saw:\n",
    "\n",
    "### $$\\Sigma_{ij}={\\rm cov}(y_i,y_j)$$\n",
    "\n",
    "We don't have a parameteric model for $y$ anymore, but that's OK, we can write down a parametric model for the covariance itself, i.e.:\n",
    "\n",
    "### $$\n",
    "\\mathrm{cov}(y(t),y(t'))=k(t,t') $$\n",
    "\n",
    "\n",
    "That's helpful to do, because with finite observations:\n",
    "\n",
    "### $$\n",
    "\\mathrm{cov}(y_i,y_j)=k(t_i,t_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So we don't have parametrized model, but do have parametrized covariance - what can we do with this thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The prior\n",
    "\n",
    "Now consider a finite set of observations: inputs $\\boldsymbol{t}$, with corresponding outputs $\\boldsymbol{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The *joint distribution* of $\\boldsymbol{y}$ given $\\boldsymbol{t}$, $m$ and $k$ is\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\mid \\boldsymbol{t},m,k) = \\mathcal{N}( \\boldsymbol{m},K),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where again, $\\boldsymbol{m}=m(\\boldsymbol{t})$ is the *mean vector* \n",
    "\n",
    "and $K$ is the *covariance matrix*, with elements $K_{ij} = k(t_i,t_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Test and training sets\n",
    "\n",
    "Suppose we have an (observed) *training set* $(\\boldsymbol{t},\\boldsymbol{y})$. \n",
    "\n",
    "We are interested in some other *test set* of inputs $\\boldsymbol{t}_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The joint distribution over the training and test sets is\n",
    "### $$\n",
    "\\mathrm{p} \\left( \\left[ \\begin{array}{l} \\boldsymbol{y} \\\\ \\boldsymbol{y}_* \\end{array} \\right] \\right) \n",
    "= \\mathcal{N} \\left( \\left[ \\begin{array}{l} \\boldsymbol{m} \\\\ \\boldsymbol{m}_* \\end{array} \\right], \n",
    "\\left[ \\begin{array}{ll} K & K_* \\\\ K_*^T & K_{**} \\end{array} \\right] \\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $\\boldsymbol{m}_* = m(\\boldsymbol{x}_*)$, $K_{**,ij} = k(t_{*,i},t_{*,j})$ and $K_{*,ij} = k(t_i,t_{*,j})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is not really any different from when we just had two observations:\n",
    "\n",
    "### $$\n",
    "\\left[ \\begin{array}{l} y_1 \\\\ y_2 \\end{array} \\right] \\sim \\mathcal{N} \\left(\n",
    "\\left[ \\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}  \\right] , \n",
    "\\left[ \\begin{array}{ll} \n",
    "\\sigma_1^2 & C \\\\\n",
    "C & \\sigma_2^2 \n",
    "\\end{array}  \\right] \n",
    "\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# For notational brevity I'm going to set the mean to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The conditional distribution\n",
    "\n",
    "The *conditional distribution* for the test set given the training set is:\n",
    "\n",
    "### $$ \n",
    "\\mathrm{p} ( \\boldsymbol{y}_* \\mid \\boldsymbol{y},k) = \\mathcal{N} ( \n",
    "K_*^T K^{-1} \\boldsymbol{y}, K_{**} - K_*^T K^{-1} K_* ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is also just a straight forward generalization from what we had with just two points:\n",
    "\n",
    "### $$\n",
    "p(y_2 \\mid y_1) = \\mathcal{N} \\left( \\mu_2 + C (y_1-\\mu_1)/\\sigma_1^2, \\sigma_2^2-C^2\\sigma_1^2 \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called the **predictive distribution**, because it can be use to predict future (or past) observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More generally, it can be used for *interpolating* the observations to any desired set of inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is one of the most widespread applications of GPs in some fields (e.g. kriging in geology, economic forecasting, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real observations always contain a component of *white noise*\n",
    "\n",
    "We need to account for this, but don't necessarily want to include in the predictions. \n",
    "\n",
    "\n",
    "If the white noise variance $\\sigma^2$ is constant, we can write \n",
    "\n",
    "### $$\n",
    "\\mathrm{cov}(y_i,y_j)=k(t_i,t_j)+\\delta_{ij} \\sigma^2,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and the conditional distribution becomes\n",
    "\n",
    "\n",
    "### $$ \n",
    "\\mathrm{p} ( \\boldsymbol{y}_* \\mid \\boldsymbol{y},k) = \\mathcal{N} ( \n",
    "K_*^T (K + \\sigma^2 \\mathbb{I})^{-1} \\boldsymbol{y}, K_{**} - K_*^T (K + \\sigma^2 \\mathbb{I})^{-1} K_* ).\n",
    "$$\n",
    "\n",
    "\n",
    "We assumed constant white noise, but it's trivial to allow for different $\\sigma$ for each data point.\n",
    "\n",
    "You could also add some intrinsic dispersion as you often have to do. \n",
    "\n",
    "In real life, we may need to learn $\\sigma_{\\text{int}}$ from the data, alongside the other contribution to the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single-point prediction\n",
    "\n",
    "Let us look more closely at the predictive distribution for a single test point $t_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is a Gaussian with mean:\n",
    "### $$\n",
    "\\overline{y}_* = \\boldsymbol{k}_*^T (K + \\sigma^2 \\mathbb{I})^{-1} \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "and variance\n",
    "### $$\n",
    "\\mathbb{V}[y_*] = k(t_*,t_*) - \\boldsymbol{k}_*^T (K + \\sigma^2 \\mathbb{I})^{-1} \\boldsymbol{k}_*,\n",
    "$$\n",
    "where $\\boldsymbol{k}_*$ is the vector of covariances between the test point and the training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice the mean is a linear combination of the observations: the GP is a *linear predictor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is also a linear combination of covariance functions, each centred on a training point:\n",
    "\n",
    "### $$\n",
    "\\overline{y}_* = \\sum_{i=1}^N \\alpha_i k(x_i,x_*),\n",
    "$$\n",
    "where $\\alpha_i = (K + \\sigma^2 \\mathbb{I})^{-1} y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"random_process.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"weight_space.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"two_views.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# So how do you choose the kernel $k$?\n",
    "\n",
    "Common choices: http://www.cs.toronto.edu/~duvenaud/cookbook/index.html\n",
    "\n",
    "Lets fiddle:\n",
    "\n",
    "# In-class exercise:\n",
    "\n",
    "[Click this](https://distill.pub/2019/visual-exploration-gaussian-processes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The likelihood\n",
    "\n",
    "The *likelihood* of the data under the GP model is simply:\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{t}) = \\mathcal{N}(\\boldsymbol{y} \\, | \\, \\boldsymbol{0},K + \\sigma^2 \\mathbb{I}).\n",
    "$$\n",
    "\n",
    "This is a measure of how well the model explains, or predicts, the training set.\n",
    "\n",
    "i.e. **The observed $\\boldsymbol{y}$ are noisy realisations of a latent (unobserved) Gaussian process $\\boldsymbol{f}$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are marginalizing over the function values $\\boldsymbol{f}$:\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{t}) = \\int \\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{f},\\boldsymbol{t}) \\, \\mathrm{p}(\\boldsymbol{f} \\,|\\, \\boldsymbol{t}) \\, \\mathrm{d}\\boldsymbol{f},\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{f} \\,|\\, \\boldsymbol{t}) = \\mathcal{N}(\\boldsymbol{f} \\, | \\, \\boldsymbol{0},K)\n",
    "$$\n",
    "\n",
    "\n",
    "is the *prior*, and \n",
    "\n",
    "\n",
    "### $$\n",
    "\\mathrm{p}(\\boldsymbol{y} \\,|\\, \\boldsymbol{f},\\boldsymbol{t}) = \\mathcal{N}(\\boldsymbol{y} \\, | \\, \\boldsymbol{0},\\sigma^2 \\mathbb{I})\n",
    "$$\n",
    "is the *likelihood*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# You \"condition\" the hyperparameters on some observed data\n",
    "\n",
    "i.e. evaluate the conditional (or predictive) distribution for a given covariance matrix (i.e. covariance function and hyper-parameters), and training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## *Training* the GP...\n",
    "\n",
    "...means maximising the *likelihood* of the model with respect to the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: \n",
    "\n",
    "[Click this](https://drafts.distill.pub/gp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The kernel trick\n",
    "\n",
    "Consider a linear basis model with arbitrarily many *basis functions*, or *features*, $\\Phi(x)$, and a (Gaussian) prior $\\Sigma_{\\mathrm{p}}$ over the basis function weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You end up with exactly the same expressions for the predictive distribution and the likelihood so long as:\n",
    "### $$\n",
    "k(\\boldsymbol{x},\\boldsymbol{x'}) = \\Phi(\\boldsymbol{x})^{\\mathrm{T}} \\Sigma_{\\mathrm{p}} \\Phi(\\boldsymbol{x'}),\n",
    "$$\n",
    "\n",
    "\n",
    "or, writing $\\Psi(\\boldsymbol{x}) = \\Sigma_{\\mathrm{p}}^{1/2} \\Phi(\\boldsymbol{x})$,\n",
    "\n",
    "\n",
    "### $$\n",
    "k(\\boldsymbol{x},\\boldsymbol{x'}) = \\Psi(\\boldsymbol{x}) \\cdot \\Psi(\\boldsymbol{x'}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Thus the covariance function $k$ enables us to go from a (finite) *input space* to a (potentially infinite) *feature space*. This is known as the *kernel trick* and the covariance function is often referred to as the *kernel*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Spitzer exoplanet transits and eclipses (Evans et al. 2015)\n",
    "\n",
    "<img src=\"Evans_Spitzer.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPs to deal with correlated noise in fitting spectra (Narayan et al., 2019)\n",
    "\n",
    "<img src=\"GP_spectra.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Mauna Kea CO$_2$ dataset\n",
    "\n",
    "(From Rasmussen & Williams textbook)\n",
    "\n",
    "<img height=\"700\" src=\"RW_mauna_kea.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPz photometric redshifts (Almosallam, Jarvis & Roberts 2016)\n",
    "\n",
    "<img src=\"Almosallam_GPz.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
