{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 14, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Supervised Learning \n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How are final projects coming along\n",
    "\n",
    "I'll try and check in on each project's git repo through this week, so actually push commits as you do things, rather than waiting right until the end.\n",
    "\n",
    "I should also have pre-final grades for you folks by Thursday (which still don't matter)\n",
    "\n",
    "Please do turn in *something* for any homework assignments in the second half of the class... (deadlines matter more than grades, but not by much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "\n",
    "- Unsupervised Learning was about finding structure in the data\n",
    "    - metric defines the distance between samples\n",
    "    - useful for clustering, density estimation, anomaly detection\n",
    "        - **all of these are effectively trying to get at classification**\n",
    "    \n",
    "- In Supervised Learning you've got labels for the data\n",
    "    - metric defines the distance between predictions for the labels and true labels\n",
    "    - This looks a lot like the first half of the semester, where you wrote down a likelihood function which defined the distance between predictions and truth \n",
    "\n",
    "- Just as with GPs we don't write down a model to predict the labels (y) given the features (x) explicitly\n",
    "    - note that this model is however implicit for some methods\n",
    "    - the goal in supervised learning is **condition/train** the usually **non-parametric model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"classification_workflow.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Individual classifiers are often **weak** (prone to bias/overfitting) so we use **ensembles of classifiers**\n",
    "- bagging (train classifiers in parallel, ensemble votes on output) \n",
    "- boosting (train classifiers in series, each one learning from the mistakes of the previous one, last classifier votes)\n",
    "\n",
    "(e.g. decision tree vs random forest)\n",
    "\n",
    "#### The stats viewpoint\n",
    "- Rather than rely solely on the structure in the data as in unsupervised learning, we're introducing extra information (**priors**) to help improve our ability to model unlabelled data (**prediction**) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance Measures/Scores (often, also called metrics, just to be confusing):\n",
    "\n",
    "\n",
    "The first question that we need to address is how we score our results - i.e. how good are our predictions\n",
    "\n",
    "In the simplest case, there are 2 types of errors:\n",
    "* a [False Positive](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error), where we have assigned a *true* class label when it is really false. \n",
    "\n",
    "This is called a \"Type-1 error\".\n",
    "\n",
    "* a [False Negative](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error), where we have assigned a *false* class label when it is really true. \n",
    "\n",
    "This is called a \"Type-II error\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All 4 [possibilities](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) for each pair of classes:\n",
    "- True Positive = **correctly identified**  (apple identified as apple)\n",
    "- True Negative = **correctly rejected**  (orange rejected as orange)\n",
    "- False Positive = **incorrectly identified**  (orange identified as apple)\n",
    "- False Negative = **incorrectly rejected**  (apple rejected as orange)\n",
    "\n",
    "In the case where there are only two classes (i.e. binary classification), these are clearly related, but in the multi-class case, these numbers tell you a more complex story:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Confusion Matrix\n",
    "\n",
    "Summarizes the \"confusion\" for the classifier. \n",
    "\n",
    "- For a perfect classifier all of the power will be along the diagonal, while confusion is represented by off-diagonal signal. \n",
    "\n",
    "Like almost everything else we have encountered during this exercise, `scikit-learn` makes it easy to compute a confusion matrix. This can be accomplished with the following: \n",
    "\n",
    "```\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "```\n",
    "\n",
    "\n",
    "Here's an example from the <a href=\"https://www.kaggle.com/c/PLAsTiCC-2018\">Photometric LSST Astronomical Time-Series Classificiation Challenge (PLaSTiCC)</a> from the winning entry by then grad student, Kyle Boone:\n",
    "<img src=\"KyleBoone_CM.png\" width=\"70%\">\n",
    "\n",
    "\n",
    "You can read about Kyle's winning entry here: https://arxiv.org/abs/1907.04690 (note that you've seen many of the classes in this CM during this semester, either on homework or in-class exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Based on TP/FP/TN/FN, we usually define either of the following pairs of terms:  \n",
    "\n",
    ">$ {\\rm completeness} = \\frac{\\rm true\\ positives}{\\rm true\\ positives + false\\ negatives}$\n",
    "\n",
    ">$  {\\rm contamination} = \\frac{\\rm false\\ positives}{\\rm true\\ positives + false\\ positives} = {\\rm false\\ discovery\\ rate}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "or\n",
    "\n",
    "> $ {\\rm true\\ positive\\ rate} = \\frac{\\rm true\\ positives} {\\rm true\\ positives + false\\ negatives}\n",
    "$\n",
    "\n",
    "> $  {\\rm false\\ positive\\ rate} = \\frac{\\rm false\\ positives} {\\rm true\\ negatives + false\\ positives} = {\\rm Type1\\ error}\n",
    "$\n",
    "\n",
    "where **completeness** = **true positive rate** and this is also called **sensitivity** or **recall**\n",
    "\n",
    "\n",
    "Which set is used is largely arbitrary and dependent on field/sub-field, but they're largely giving you similar information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly \n",
    " \n",
    ">$ {\\rm efficiency} = 1 - {\\rm contamination} = {\\rm precision}. $\n",
    "\n",
    "Scikit-Learn also reports the **F1 score** which is the harmonic mean (i.e. reciprocal of arithmetic mean of reciprocals) of precision and sensitivity (efficiency and completeness).\n",
    "\n",
    "Depending on your goals, you may want to maximize the completeness or the efficiency, or a combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifier performance can also be viewed as a tradeoff \n",
    "\n",
    "You might want to minimize voter fraud (contamination), but if doing so reduced voter participation (completeness) by a larger amount, then that wouldn't be such a good thing.  \n",
    "\n",
    "So you need to decide what balance you want to strike.\n",
    "\n",
    "**Note that this tradeoff is different from the bias/variance tradeoff.** \n",
    "You can still infer bias results from a sample without much contamination!\n",
    "\n",
    "If your labels were positives = detected, and negatives = below detection threshold, you can make the analogy to HW8, where you looked at completeness and efficiency for a simulated survey - what fraction of sources you recovered, and how efficient you were at discriminating sources from background noise excursion. \n",
    "\n",
    "You had a hard cut in number of counts (a threshold) so you had a sample that was not contaminated, but if you only used the detected objects for inference you'd still get a biased value for population parameters (exponent of the power law)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing the performance of classifiers\n",
    "\n",
    "So, \"best\" performance is a bit of a subjective topic. \n",
    "- We trade contamination as a function of completeness and this is science dependent.\n",
    "\n",
    "This is true even for just a single classifier, if we get a score (or probability) for an object being a certain class. Then we don't have a sharp decision boundary but rather a threshold that we can vary.\n",
    "\n",
    "This choice of threshold impacts the TPR/FPR so we want to characterize how it impacts classification. The way that we will do this is with a [**Receiver Operating Characteristic (ROC)**](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve.  \n",
    "\n",
    "A ROC curve simply plots the true-positive vs. the false-positive rate as a function of threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ROC curves \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"500px\">\n",
    "            <img src=\"roc_schematic.jpg\">\n",
    "        </td>\n",
    "        <td width=\"500px\">\n",
    "            <img src=\"roc_plasticc.jpg\">\n",
    "        </td>\n",
    "     </tr>\n",
    "</table>\n",
    "\n",
    "Annoyingly, they're only well defined for binary classification, so if you have a multi-class problem, then you have to:\n",
    "\n",
    "- plot 1 ROC curve per class\n",
    "- **micro-averaging**: Reduce multi-class problem to correctly classififed/not-correctly classified regardless of class\n",
    "- **macro-averaging**: plot average ROC curve across all classes (what's on the right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AUC\n",
    "\n",
    "Generally speaking, you want to chose a classifier that maximizes the **area under the curve.**\n",
    "- very literally the area under the ROC curve\n",
    "- classifiers with a higher AUC are better (more true positives, less false positives)\n",
    "\n",
    "## Precision-Recall curves\n",
    "\n",
    "You can also plot completeness/contamination or precision/recall.\n",
    "\n",
    "One concern about ROC curves is that they are sensitive to the relative sample sizes.\n",
    "- If there are many more background events than source events small false positive results can dominate a signal. \n",
    "- For these cases we can plot efficiency (1 - contamination) vs. completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"PRC_turbo_3_downsampled_thresh_lines.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Actually computing performance scores:\n",
    "\n",
    "- [sklearn.metrics.roc_curve(y_test, y_prob)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "- [sklearn.metrics.precision_recall_curve(y_test, y_prob)](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html)\n",
    "- astroML.utils.completeness_contamination(y_pred, y_test)\n",
    "\n",
    "**Note** \n",
    "- [`sklearn.metrics` algorithms](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) take `y_test`, which are classes, and `y_prob`, which are not class predictions, but rather probabilities\n",
    "- the AstroML algorithm wants `y_pred` (which we get by converting `y_prob` into discrete predictions as a function of the probability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Different Supervised Learning methods:\n",
    "\n",
    "\n",
    "With Unsupervised Learning, there were a few different approaches-\n",
    "- centroid-based (k-means/median)\n",
    "- distribution-based (GMMs/Extreme Deconvolution)\n",
    "- linkage/connectivity-based (hierarchical clustering/isolation forests)\n",
    "- density-based (KDEs/DBSCAN/Optics)\n",
    "\n",
    "We can divide these methods into two groups-\n",
    "\n",
    "## Generative vs. Discriminative Classification\n",
    "\n",
    "As an example, if you are trying to determine whether a galaxy is at redshift 0.2 or redshift 2, you could either \n",
    "- learn how galaxies are distributed over all redshifts and given this model, you can evaluate the likelihood ratio of the data at both redshifts\n",
    "- OR, you can learn what key features discriminate between galaxies at z=0.2 and z=2\n",
    "\n",
    "For example, in the figure below, to classify a new object with $x=1$, it would suffice to know that either \n",
    "1) model 1 is a better fit than model 2\n",
    "or \n",
    "2) $x=1.4$ is a good discriminator between the two populations \n",
    "\n",
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we find ourselves asking which category is most likely to generate the observed result, then we are using using **density estimation** for classification \n",
    "- this is referred to as **generative classification**.  \n",
    "- we have a full model of the density for each class or we have a model which describes how data could be generated from each class\n",
    "- both distribution-based (where you assume a form the density) and density-based (where you directly estimate the density from the data) approaches are here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we don't care about the full distribution, then we are doing something more like clustering\n",
    "- we don't need to map the distribution, we just need to define boundaries.  \n",
    "- Classification that finds the **decision boundary** that separates classes is called **discriminative classification**\n",
    "- centroid-based and linkage-based methods come under this class\n",
    "- For high-dimensional data, this may be a better choice\n",
    "    - usual reason - curse of dimensionality\n",
    "    \n",
    "You've already seen **decision trees** and **random forests** as examples of discriminative classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)\n",
    "\n",
    "In reality, we usually do both. \n",
    "\n",
    "We first do discriminative classification using a decision boundary based-method, and then we do generative classification using density estimation for the class of interest (in order to determine a probability of group membership)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Classification\n",
    "\n",
    "We can use Bayes' theorem to relate the labels to the features in an $N\\times D$ data set $X$.  \n",
    "\n",
    "The $j$th feature of the $i$th sample is $x_{ij}$ and there are $k$ classes giving discrete labels $y_k$.  \n",
    "Then we have:\n",
    "\n",
    "$$p(y_k|x_i) = \\frac{p(x_i|y_k)p(y_k)}{\\sum_i p(x_i|y_k)p(y_k)}$$\n",
    "\n",
    "where $x_i$ is a vector with $j$ components.\n",
    "\n",
    "$p(y=y_k)$ is the probability of any point having class $k$ (equivalent to the prior probability of the class $k$). \n",
    "\n",
    "In generative classifiers we model class-conditional densities $p_k(x) = p(x|y=y_k)$ and our goal is to estimate the $p_k$'s. \n",
    "\n",
    "Before we get into the generative classification algortithms, we'll first discuss 3 general concepts:\n",
    "- Discriminant Functions\n",
    "- Bayes Classifiers\n",
    "- Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Discriminant Function\n",
    "\n",
    "We can relate classification to density estimation and regression.\n",
    "\n",
    "$\\hat{y} = f(y|x)$ represents the best guess of $y$ given $x$. \n",
    "\n",
    "Classification can be thought of as the analog of regression where $y$ is a discrete *category* rather than a continuous variable, for example $y=\\{0,1\\}$.\n",
    "\n",
    "(Alternately regression is just classification with very many classes distrbuted on a real number line).\n",
    "\n",
    "In classification we refer to $f(y|x)$ as the [**discriminant function**](https://en.wikipedia.org/wiki/Discriminant_function_analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For a simple 2-class example, if you want the best guess of the label y:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "g(x) = f(y|x) & = &  \\int y \\, p(y|x) \\, dy \\\\\n",
    "%    & = & \\int y p(y|x) \\, dy \\\\\n",
    "       & = & 1 \\cdot p(y=1 | x) + 0 \\cdot p(y=0 | x) = p(y=1 | x).\n",
    "%     & = & p(y=1 | x)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "From Bayes rule (yet again):\n",
    "\n",
    "$$g(x) = \\frac{p(x|y=1) \\, p(y=1)}{p(x|y=1) \\, p(y=1)  + p(x|y=0) \\, p(y=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes Classifier\n",
    "\n",
    "If the discriminant function gives a binary prediction, we call it a **Bayes classifier**, formulated as\n",
    "\n",
    "\n",
    "$$\\begin{eqnarray} \\widehat{y} & = & \\left\\{ \\begin{array}{cl}       \t           1 & \\mbox{if $g(x) > 1/2$}, \\\\       \t           0 & \\mbox{otherwise,}       \t           \\end{array}     \t   \\right. \\\\     & = & \\left\\{\n",
    "\\begin{array}{cl}               1 & \\mbox{if $p(y=1|x) > p(y=0|x)$}, \\\\               0 & \\mbox{otherwise.}               \\end{array}       \\right.\\end{eqnarray}$$\n",
    "\n",
    "**i.e. the best guess class is the most likely** (duh)\n",
    "\n",
    "This can be generalized to any number of classes, $k$, and not just two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decision Boundary\n",
    "\n",
    "The **decision boundary** is just the set of $x$ values at which each class is equally likely:\n",
    "\n",
    "$$\n",
    "p(x|y=1)p(y=1)  =  p(x|y=0)p(y=0);\n",
    "$$\n",
    "\n",
    "$$g_1(x) = g_2(x) \\; {\\rm or}\\; g(x) = 1/2$$\n",
    "\n",
    "So same figure as earlier - we're just assigning classifications according to which pdf is higher at every given $x$.\n",
    "\n",
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplest Classifier: Naive Bayes\n",
    "\n",
    "In practice classification can be very complicated as the data are generally multi-dimensional (that is we don't just have $x$, we have $x_{j=0},x_1,x_2,x_3...x_n$, so we want $p(x_0,x_1,x_2,x_3...x_n|y)$.\n",
    "\n",
    "However, if we **assume** that all attributes are conditionally independent (which is not always true, but is often close enough), then this simplifies to\n",
    "\n",
    "$$ p(x_1,x_2|y_k) = p(x_1|y)p(x_2|y_k)$$\n",
    "  \n",
    "which can be written as\n",
    "\n",
    "$$ p({x_{j=0},x_1,x_2,\\ldots,x_N}|y_k) = \\prod_j p(x_j|y_k).$$\n",
    "\n",
    "From Bayes' rule and conditional independence we get\n",
    "\n",
    "$$\n",
    "  p(y_k | {x_0,x_1,\\ldots,x_N}) =\n",
    "  \\frac{\\prod_j p(x_j|y_k) p(y_k)}\n",
    "       {\\sum_l \\prod_j p(x_j|y_l) p(y_l)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We calculate the most likely value of $y$ by maximizing over $y_k$:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{y_k} \\frac{\\prod_j p(x_j|y_k) p(y_k)}\n",
    "        {\\sum_l \\prod_j p(x_j|y_l) p(y_l)},\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "From there the process is just estimating densities: $p(x|y=y_k)$ and $p(y=y_k)$ are learned from a set of training data, where\n",
    "- $p(y=y_k)$ is just the frequency of the class $k$ in the training set\n",
    "- $p(x|y=y_k)$ is just the density (probability) of an object with class $k$ having the attributes $x$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "\n",
    "Of course, to get $p(x|y=y_k)$ you can use all of the density estimation methods from last week. \n",
    "\n",
    "The parametric model is to assert that we have a bunch of 1-D Gaussians i.e. $p(x_i|y=y_k)$ is a normal distributions, with means $\\mu_{ik}$ and widths $\\sigma_{ik}$. \n",
    "\n",
    "The naive Bayes estimator is then\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_{y_k}\\left[\\ln p(y=y_k) - \\frac{1}{2}\\sum_{i=1}^N\\left(2\\pi(\\sigma_{ik})^2 + \\frac{(x_i - \\mu_{ik})^2}{(\\sigma_{ik})^2} \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"naive\" refers to the fact that we are assuming that all of the variable are independent. You can of course compute the covariance matrix to estimate the degree to which this is bullshit.\n",
    "\n",
    "But then you can use dimensionality reduction like PCA to help \"whiten\" the features and construct independent variables.\n",
    "\n",
    "Generally, naieve Bayes by itself is a bad idea because of the covariances, but PCA might also be a bad idea because it's a linear transformation and the covariance may not be linear.\n",
    "\n",
    "If instead, we relax that assumption and allow for covariances then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Linear Discriminant Analysis\n",
    "\n",
    "In [Linear Discriminant Analysis (LDA)](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) we assume that the class distributions have identical covariances for all $k$ classes (all classes are a set of shifted Gaussians). \n",
    "\n",
    "The class-dependent covariances that would normally give rise to a quadratic dependence on\n",
    "$X$ cancel out if they are assumed to be constant. \n",
    "\n",
    "The Bayes classifier is, therefore, linear with respect to $X$, and  discriminant boundary between classes is the line that minimizes the overlap between Gaussians.\n",
    "\n",
    "Basically, we're projecting N-dimensional data onto k new axes and finding what the optimal decision boundary is for each. Therefore LDA is also used for dimensionality reduction, but the new features may still be correlated.\n",
    "\n",
    "<img src=\"lda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Relaxing the requirement that the covariances of the Gaussians are constant, the discriminant function becomes quadratic in $X$.\n",
    "\n",
    "This is sometimes known as [Quadratic Discriminant Analysis (QDA)](https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis).\n",
    "\n",
    "[`LDA`](http://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html) and [`QDA`](http://scikit-learn.org/0.16/modules/generated/sklearn.qda.QDA.html#sklearn.qda.QDA) are implemented in Scikit-Learn as follows and an example using the same data as above is given below.\n",
    "\n",
    "<img src=\"lda_vs_qda.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Nearest Neighbor Classifier\n",
    "\n",
    "These approaches are using a model for the population density. We can, just as in unsupervised learning, use linkage instead (or local density):\n",
    "\n",
    "[$k$-nearest-neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) ($k$NN) is simple and can be used for clasification and regression. \n",
    "\n",
    "The output is determined by examining the $k$ nearest neighbors in the training set, where $k$ is a user defined number. \n",
    "\n",
    "Typically, though not always, distances between sources are Euclidean, and the final classification is assigned to whichever class has a plurality within the $k$ nearest neighbors (in the case of regression, the average of the $k$ neighbors is the output from the model). \n",
    "\n",
    "The number of neighbors, $k$, regulates the complexity of the classification, where a larger $K$ decreases the variance in the classification but leads to an increase in the bias.  \n",
    "\n",
    "The distance measure is usually N-D Euclidean.  However, if the attributes have very different properties, then normalization, weighting, etc. may be needed.\n",
    "\n",
    "<img src=\"knn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In-class exercise: Putting together the full supervised-learning workflow\n",
    "\n",
    "Data comes from [this paper.](https://ui.adsabs.harvard.edu/abs/2010ApJS..186..427N/abstract) where Preethi Nair (LSU) heroically visually classified 14,000 galaxies.\n",
    "\n",
    "Her T-Type morphology is much more fine-grained but it can be mapped to broad galaxy morphology.\n",
    "\n",
    "T-Type:\n",
    "- Ellipticals: -6 to -4  \n",
    "- Spirals: -3 to 7  \n",
    "- Irregular things: 7 and up\n",
    "\n",
    "So the morphology column in the file is just a letter: \n",
    "- Spiral: S \n",
    "- Elliptical: E \n",
    "- Other: N\n",
    "\n",
    "And that in turn, can be binarized as Ellipticals =1, Not Ellipticals = 0\n",
    "\n",
    "\n",
    "\n",
    "## 1. Clean the Data:\n",
    "\n",
    "Only learn on well defined morphologies, aka elliptical or spirals.\n",
    "\n",
    "## 2. Preprocess:\n",
    "\n",
    "The dataset incorporates lots of features, but there's also columns that give away the answer\n",
    "\n",
    "Therefore only use these:\n",
    "```\n",
    "feature_names = ['zs', 'g_mag', 'r_mag', 'sigma', 'M_L', 'SFRT', 'b_a', 'log_M_']\n",
    "```\n",
    "\n",
    "Normalize the features (you can try `MinMaxScaler` instead of the `StandardScaler`).\n",
    "Then split the data into a training and testing set (\n",
    "\n",
    "You may also want to encode the labels:\n",
    "\n",
    "```\n",
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(y)\n",
    "```\n",
    "\n",
    "## 3. Build models\n",
    "\n",
    "You already know how to use the Random Forest. Now try LDA and kNN.\n",
    "\n",
    "\n",
    "## 4. Evaluate performance\n",
    "\n",
    "Since we made this a two class problem, plot the confusion matrix and the ROC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import astropy.table as at\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<i>Table length=14034</i>\n",
       "<table id=\"table112097232784\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>SDSS</th><th>zs</th><th>g_mag</th><th>r_mag</th><th>Rp</th><th>log_M_</th><th>Age</th><th>SFRT</th><th>SFRM</th><th>mug</th><th>M_L</th><th>b_a</th><th>sigma</th><th>e_sigma</th><th>TT</th><th>RA</th><th>DEC</th><th>Morphology</th><th>Morphology_i</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th>mag</th><th>mag</th><th>kpc</th><th>[solMass]</th><th>Gyr</th><th>solMass / yr</th><th>1 / yr</th><th>mag / arcsec2</th><th>Sun</th><th></th><th>km / s</th><th>km / s</th><th></th><th>deg</th><th>deg</th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str20</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>float32</th><th>int32</th><th>float32</th><th>float32</th><th>str1</th><th>int64</th></tr></thead>\n",
       "<tr><td>J155341.74-003422.84</td><td>0.078</td><td>15.82</td><td>15.058</td><td>18.669</td><td>11.083</td><td>4.459</td><td>1.014</td><td>-9.958</td><td>22.631</td><td>0.194</td><td>0.794</td><td>143.68</td><td>7.89</td><td>3</td><td>238.42392</td><td>-0.573011</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J155146.83-000618.62</td><td>0.055</td><td>15.512</td><td>14.606</td><td>12.185</td><td>11.245</td><td>7.111</td><td>0.896</td><td>-10.94</td><td>22.439</td><td>0.329</td><td>0.954</td><td>204.81</td><td>5.36</td><td>-5</td><td>237.94511</td><td>-0.105172</td><td>E</td><td>1</td></tr>\n",
       "<tr><td>J154453.22+002415.48</td><td>0.034</td><td>15.631</td><td>14.838</td><td>6.12</td><td>10.405</td><td>4.287</td><td>-0.012</td><td>-10.835</td><td>22.47</td><td>0.094</td><td>0.848</td><td>129.97</td><td>5.46</td><td>-2</td><td>236.22176</td><td>0.4043</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J154711.32+002424.81</td><td>0.033</td><td>15.716</td><td>15.158</td><td>11.094</td><td>10.156</td><td>1.901</td><td>1.109</td><td>-9.926</td><td>22.631</td><td>0.045</td><td>0.854</td><td>45.25</td><td>12.27</td><td>4</td><td>236.79716</td><td>0.406892</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J154514.39+004619.89</td><td>0.013</td><td>15.341</td><td>14.956</td><td>6.917</td><td>9.173</td><td>1.891</td><td>-1.738</td><td>-10.288</td><td>22.533</td><td>-0.077</td><td>0.329</td><td>89.32</td><td>14.59</td><td>5</td><td>236.30997</td><td>0.772192</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J155255.43+004304.87</td><td>0.033</td><td>15.86</td><td>15.084</td><td>3.46</td><td>10.484</td><td>6.842</td><td>0.437</td><td>-11.138</td><td>22.129</td><td>0.265</td><td>0.76</td><td>188.82</td><td>5.21</td><td>-5</td><td>238.23096</td><td>0.718019</td><td>E</td><td>1</td></tr>\n",
       "<tr><td>J155357.40+004117.11</td><td>0.039</td><td>15.784</td><td>15.147</td><td>6.882</td><td>10.627</td><td>2.004</td><td>0.461</td><td>-9.978</td><td>22.174</td><td>0.273</td><td>0.486</td><td>102.61</td><td>7.84</td><td>1</td><td>238.48917</td><td>0.688086</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J110122.00-010824.89</td><td>0.074</td><td>15.588</td><td>14.628</td><td>17.776</td><td>11.442</td><td>7.491</td><td>0.176</td><td>-11.327</td><td>22.463</td><td>0.28</td><td>0.755</td><td>285.78</td><td>6.63</td><td>-5</td><td>165.34167</td><td>-1.140247</td><td>E</td><td>1</td></tr>\n",
       "<tr><td>J112000.06-010711.96</td><td>0.025</td><td>15.97</td><td>15.892</td><td>3.131</td><td>9.119</td><td>0.503</td><td>0.171</td><td>-9.034</td><td>21.056</td><td>-0.389</td><td>0.562</td><td>40.34</td><td>17.84</td><td>0</td><td>170.00024</td><td>-1.119989</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>J131146.22+143413.48</td><td>0.027</td><td>15.764</td><td>15.034</td><td>2.47</td><td>10.287</td><td>0.0</td><td>-2.031</td><td>-11.864</td><td>22.124</td><td>0.0</td><td>0.825</td><td>128.75</td><td>3.1</td><td>0</td><td>197.94258</td><td>14.570411</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J131952.50+142325.08</td><td>0.071</td><td>15.674</td><td>15.274</td><td>11.607</td><td>10.743</td><td>1.855</td><td>0.168</td><td>-9.735</td><td>21.747</td><td>-0.03</td><td>0.877</td><td>62.97</td><td>7.55</td><td>4</td><td>199.96875</td><td>14.3903</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J132155.38+141957.80</td><td>0.024</td><td>15.225</td><td>14.618</td><td>9.272</td><td>10.026</td><td>2.727</td><td>-1.103</td><td>-9.869</td><td>22.737</td><td>0.052</td><td>0.488</td><td>28.01</td><td>42.59</td><td>5</td><td>200.48074</td><td>14.332722</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J130800.20+150102.07</td><td>0.064</td><td>15.961</td><td>15.366</td><td>18.042</td><td>10.898</td><td>3.222</td><td>0.277</td><td>-9.676</td><td>22.683</td><td>0.237</td><td>0.668</td><td>82.3</td><td>8.66</td><td>4</td><td>197.00082</td><td>15.017242</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J131901.87+144727.65</td><td>0.023</td><td>15.105</td><td>14.551</td><td>5.941</td><td>10.1</td><td>2.118</td><td>-0.672</td><td>-9.698</td><td>22.164</td><td>0.047</td><td>0.52</td><td>55.85</td><td>7.56</td><td>5</td><td>199.7578</td><td>14.791014</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J124719.07+154235.64</td><td>0.069</td><td>15.786</td><td>14.906</td><td>16.597</td><td>11.258</td><td>7.986</td><td>-0.217</td><td>-10.721</td><td>22.535</td><td>0.305</td><td>0.565</td><td>195.68</td><td>6.66</td><td>2</td><td>191.82945</td><td>15.7099</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J130510.16+152606.67</td><td>0.053</td><td>15.318</td><td>14.598</td><td>10.95</td><td>10.935</td><td>4.255</td><td>-0.052</td><td>-10.182</td><td>22.354</td><td>0.187</td><td>0.707</td><td>121.74</td><td>5.39</td><td>2</td><td>196.29233</td><td>15.435186</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J131525.21+152522.23</td><td>0.027</td><td>15.255</td><td>14.349</td><td>5.514</td><td>10.742</td><td>0.0</td><td>-1.075</td><td>-11.132</td><td>22.265</td><td>0.0</td><td>0.349</td><td>243.68</td><td>4.62</td><td>1</td><td>198.85504</td><td>15.422842</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J132045.57+151532.75</td><td>0.023</td><td>15.724</td><td>14.936</td><td>5.177</td><td>10.25</td><td>4.399</td><td>-0.534</td><td>-9.987</td><td>22.781</td><td>0.337</td><td>0.609</td><td>65.21</td><td>5.18</td><td>0</td><td>200.18987</td><td>15.259097</td><td>S</td><td>0</td></tr>\n",
       "<tr><td>J132135.96+151917.80</td><td>0.023</td><td>15.471</td><td>14.805</td><td>3.879</td><td>10.313</td><td>5.76</td><td>-0.848</td><td>-10.41</td><td>22.171</td><td>0.301</td><td>0.45</td><td>120.39</td><td>3.95</td><td>3</td><td>200.39983</td><td>15.321611</td><td>S</td><td>0</td></tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Table length=14034>\n",
       "        SDSS            zs    g_mag  ...    DEC    Morphology Morphology_i\n",
       "                               mag   ...    deg                           \n",
       "       str20         float32 float32 ...  float32     str1       int64    \n",
       "-------------------- ------- ------- ... --------- ---------- ------------\n",
       "J155341.74-003422.84   0.078   15.82 ... -0.573011          S            0\n",
       "J155146.83-000618.62   0.055  15.512 ... -0.105172          E            1\n",
       "J154453.22+002415.48   0.034  15.631 ...    0.4043          S            0\n",
       "J154711.32+002424.81   0.033  15.716 ...  0.406892          S            0\n",
       "J154514.39+004619.89   0.013  15.341 ...  0.772192          S            0\n",
       "J155255.43+004304.87   0.033   15.86 ...  0.718019          E            1\n",
       "J155357.40+004117.11   0.039  15.784 ...  0.688086          S            0\n",
       "J110122.00-010824.89   0.074  15.588 ... -1.140247          E            1\n",
       "J112000.06-010711.96   0.025   15.97 ... -1.119989          S            0\n",
       "                 ...     ...     ... ...       ...        ...          ...\n",
       "J131146.22+143413.48   0.027  15.764 ... 14.570411          S            0\n",
       "J131952.50+142325.08   0.071  15.674 ...   14.3903          S            0\n",
       "J132155.38+141957.80   0.024  15.225 ... 14.332722          S            0\n",
       "J130800.20+150102.07   0.064  15.961 ... 15.017242          S            0\n",
       "J131901.87+144727.65   0.023  15.105 ... 14.791014          S            0\n",
       "J124719.07+154235.64   0.069  15.786 ...   15.7099          S            0\n",
       "J130510.16+152606.67   0.053  15.318 ... 15.435186          S            0\n",
       "J131525.21+152522.23   0.027  15.255 ... 15.422842          S            0\n",
       "J132045.57+151532.75   0.023  15.724 ... 15.259097          S            0\n",
       "J132135.96+151917.80   0.023  15.471 ... 15.321611          S            0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = at.Table.read('SDSS_morphology.fit')\n",
    "data.convert_bytestring_to_unicode()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
