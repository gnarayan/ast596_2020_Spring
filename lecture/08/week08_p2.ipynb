{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 08, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Time-series Analysis, p2\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>\n",
    "\n",
    "Borrowing heavily from Gordon Richards (Drexel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Midterm solutions posted </center>\n",
    "\n",
    "\n",
    "### <center> Policy Updates from the University </center>\n",
    "<center> The deadline to drop a full-semester course is now April 30. </center>\n",
    "\n",
    "<center> Students may select to take any course as credit/no-credit (CR/NC).</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Quick Review of Midterm </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "- Measurements  we looked at in the first half of this semester were all independent and identically distributed (i.i.d) \n",
    "\n",
    "# $$ P(x_1 \\cap x_2) = P(x_1)\\cdot P(x_2) $$\n",
    "\n",
    "- You explictly **CANNOT** make this assumption with time-series data\n",
    "    - Observations at time $t_n$ are **correlated** with potentially all previous observations\n",
    "    - This **autocorrelation structure** is precisely what we are interested in\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The autocorrelation function \n",
    "\n",
    "Same as defintiion in week 6 for Markov Chains:\n",
    "\n",
    "# $$\\rho_X(k) = \\frac{\\text{Cov}(X_i, X_{i+k})}{\\text{Var}(X_i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The autocorrelation function \n",
    "\n",
    "Simple illustration - two apparently random numbers:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"acf.png\" width=30%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Blue is really $ y \\sim 2\\sin(5t) + N(0, 1)$ while orange is just $y \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If a the joint probability distribution of the measurements of the time-series does not change when shifted in time it is **stationary** - we used this term when we talked about Markov Chains, but there's a subtlety, because we don't mean exactly the same thing as in that context.\n",
    "\n",
    "# 1. **Strict stationarity** \n",
    "   \n",
    " \n",
    "## $$ X(t_1), X(t_2),...,X(t_n) = X(t_1+\\tau), X(t_2+\\tau),...,X(t_n+\\tau) $$\n",
    "\n",
    "for all $t_i$, all $n$ **and** all lags $\\tau$\n",
    "\n",
    "- Time-series which are stationary have autocorrelations that can only depend on the difference between two times i.e. $|t_i - t_j|$ rather than the actual coordinate value \n",
    "\n",
    "- This means that properties like the mean and variance of the process are constant with time\n",
    "    - This is why we want our Markov Chains to be stationary\n",
    "   \n",
    "### A **strict stationary** process makes for a boring time-series because the moments are constant \n",
    "(note that constant doesn't mean finite - you could draw random samples and index them from a Cauchy distribution - the variance wouldn't be finite, but it is a strictly stationary process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# 2. **Weak stationarity** \n",
    "\n",
    "- The first moment and the autocorrelation function need to be constant with time, and the variance must be finite\n",
    "    - The autocorrelation must be constant with time != the autocorrelation must be the same for all lags\n",
    "    - In particular, if there's a set of peaks in the autocorrelation function for specific lags $|t_i - t_j|$, that autocorrelation structure itself cannot change with time\n",
    "        - Strictly stationary processes with finite variance are also weakly stationary, but not vice-versa\n",
    "        \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"example_ts.jpg\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "* a = strong stationary\n",
    "* b = non-stationary but maybe can be detrended and transformed into c\n",
    "* c = weak stationary \n",
    "\n",
    "# [More detailed description of stationarity in time-series](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322)\n",
    "\n",
    "### If we say **stationary** and we're in the context of time-series, we do not mean strictly stationary as was the case with Markov Chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last class we looked at a radial velocity system\n",
    "\n",
    "- you can write down a nice parametric model\n",
    "    - i.e. there are analytic expressions for means and variances\n",
    "*AND*\n",
    "- the correlations between observations at $t_i$ and $t_i+\\tau$ can be used to constrain the parameters of the model\n",
    "    - i.e. the autocorrelation structure is special - **cyclostationary** \n",
    "        - repeats with integer multiples of the period - the underlying process is continious \n",
    "    \n",
    "    \n",
    "Because we a) had a model and b) the observations had strong correlations (you estimated the period reasonably by eye!)\n",
    "- if you are lucky enough to have a model\n",
    "- and can write down likelihood and priors\n",
    "then you get to use all the machinery from the first half of this semester to infer the correlation structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general, you don't get this lucky. You don't have a simple model and the correlation structure isn't this nice.\n",
    "\n",
    "# What we are going to cover next:\n",
    "\n",
    "We'll remove each of these conditions one at a time.\n",
    "Today:\n",
    "- simple strong correlation, no parametric model, stationary, continious (e.g. cepheids)\n",
    "- complex correlation, no parametric model, stationary, continious (e.g. sunspots)\n",
    "    - non-stationary, continious (e.g. AGN) aren't really much different\n",
    "\n",
    "Next week:    \n",
    "- discontinious (e.g. Supernovae <3<3<3 - these are the bestest!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fourier Analysis\n",
    "\n",
    "You can express any periodic signal as a sum of sines within noise, given sufficiently many components $M$ \n",
    "\n",
    "## $$y_i(t_i) = Y_o + \\sum_{m=1}^M \\beta_m \\sin(m \\omega t_i + \\phi_m)   + \\epsilon_i$$\n",
    "\n",
    "\n",
    "Here's an illustration from the ICVG text of reconstructing a model RR Lyrae lightcurve with $k$ sine waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import astroML.datasets.rrlyrae_templates\n",
    "from astroML.datasets import fetch_rrlyrae_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# RUN THIS\n",
    "\n",
    "# Load the RR Lyrae template\n",
    "templates = fetch_rrlyrae_templates()\n",
    "x, y = templates['115r'].T\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=0)\n",
    "\n",
    "kvals = [1, 3, 8]\n",
    "subplots = [311, 312, 313]\n",
    "\n",
    "for (k, subplot) in zip(kvals, subplots):\n",
    "    ax = fig.add_subplot(subplot)\n",
    "\n",
    "    # Use FFT to fit a truncated Fourier series\n",
    "    # reconstruct using k frequencies\n",
    "    y_fft = np.fft.fft(y) # compute FFT\n",
    "    y_fft[k + 1:-k] = 0 # zero-out frequencies higher than k\n",
    "    y_fit = np.fft.ifft(y_fft).real # reconstruct using k modes\n",
    "\n",
    "    # plot the true value and the k-term reconstruction\n",
    "    ax.plot(np.concatenate([x, 1 + x]),\n",
    "            np.concatenate([y, y]), '--k', lw=2)\n",
    "    ax.plot(np.concatenate([x, 1 + x]),\n",
    "            np.concatenate([y_fit, y_fit]), color='gray')\n",
    "\n",
    "    label = \"%i mode\" % k\n",
    "    if k > 1:\n",
    "        label += 's'\n",
    "\n",
    "    ax.text(0.02, 0.1, label, ha='left', va='bottom',\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "    if subplot == subplots[-1]:\n",
    "        ax.set_xlabel('phase')\n",
    "    else:\n",
    "        ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    if subplot == subplots[1]:\n",
    "        ax.set_ylabel('amplitude')\n",
    "    ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "    ax.set_xlim(0, 2)\n",
    "    ax.set_ylim(1.1, -0.1)\n",
    "\n",
    "fig = plt.gcf();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Fourier Transform by itself can be powerful if \n",
    "1. the signal-to-noise is high\n",
    "2. the signal is continious and uniformly sampled \n",
    "3. the shape you are modeling is simple and can be decomposed into a few Fourier terms.\n",
    "\n",
    "These are not the usual conditions we have when taking data. \n",
    "\n",
    "The figure on the left is the kind of data that you **want** to have, whereas the figure on the right is the kind of data that you are more likely to actually have.\n",
    "<img src=\"rrlyrae-good.png\" style=\"float: left; width: 40%; margin-right: 1%;\"> <img src=\"rrlyrae-bad.png\" style=\"float: left; width: 40%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Periodogram\n",
    "\n",
    "What we want to be able to do is to detect variability and measure the period in the face of both noisy and incomplete data. Instead we'll use Fourier decomposition to get a more useful tool for actual data analysis.\n",
    "\n",
    "\n",
    "or a periodic signal we have:\n",
    "\n",
    "$$y(t+P)=y(t),$$ where $P$ is the period.\n",
    "\n",
    "We can create a *phased light curve* that plots the data as function of phase:\n",
    "$$\\phi=\\frac{t}{P} − {\\rm int}\\left(\\frac{t}{P}\\right),$$\n",
    "\n",
    "where ${\\rm int}(x)$ returns the integer part of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A Single Sinusoid\n",
    "\n",
    "Let's take the case where the data are drawn from a single sinusoidal signal:\n",
    "\n",
    "# $$y(t)=A \\sin(\\omega t+\\phi)+\\epsilon$$\n",
    "\n",
    "and determine whether or not the data are indeed consistent with periodic variability and, if so, what is the period.\n",
    "\n",
    "\n",
    "This model is annoying to work with because it's **non-linear** in the frequency term, $\\omega$ and the phase, $\\phi$\n",
    "\n",
    "We can rewrite the argument as $\\omega(t−t_0)$ (reexpressing the phase term) and use trig identies to rewrite the model as: \n",
    "\n",
    "# $$y(t)=a \\sin(\\omega t)+b \\cos(\\omega t)$$\n",
    "\n",
    "where \n",
    "\n",
    "# $$A=(a^2+b^2)^{1/2} \\text{ and } \\phi=\\tan^{−1}(b/a)$$\n",
    "\n",
    "The model is now linear with respect to coefficients $a$ and $b$ (and nonlinear only with respect to frequency, $\\omega$). So we got rid of one of the two non-linear parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assuming constant uncertainties on the data, we can write a likelihood function down:\n",
    "\n",
    "### $$L =\\prod^N_{j=1}\\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(\\frac{−[y_j−a \\sin(\\omega t_j)−b \\cos(\\omega t_j)]^2}{2\\sigma^2} \\right) $$\n",
    "\n",
    "where $y_i$ is the measurement (e.g., the brightness of a star) taken at time $t_i$.\n",
    "\n",
    "### And now there's a bunch of math...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Assuming uniform priors on $a, b, \\omega$, and $\\sigma$ (which gives nonuniform priors on $A$ and $\\phi$) the posterior can be simplified to:\n",
    "\n",
    "# $$p(\\omega,a,b,\\sigma|{t,y}) \\propto \\sigma^{−N} \\exp \\left(\\frac{−NQ}{2\\sigma^2} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "with\n",
    "\n",
    "> $Q= V - {2\\over N} \\left[ a \\, I(\\omega) + b \\, R(\\omega) - a\\, b\\, M(\\omega) - {1 \\over 2} a^2 \\, S(\\omega) - {1 \\over 2} b^2 \\,C(\\omega)\\right]$\n",
    "\n",
    "and\n",
    "\n",
    "> $            V = {1\\over N} \\sum_{j=1}^N y_j^2$\n",
    "\n",
    "> $       I(\\omega) = \\sum_{j=1}^N y_j   \\sin(\\omega t_j)$\n",
    "\n",
    "> $ R(\\omega) = \\sum_{j=1}^N y_j  \\cos(\\omega t_j)$\n",
    "\n",
    "> $      M(\\omega) = \\sum_{j=1}^N \\sin(\\omega t_j) \\, \\cos(\\omega t_j)$\n",
    "\n",
    "> $      S(\\omega) = \\sum_{j=1}^N \\sin^2(\\omega t_j)$\n",
    "\n",
    "> $ C(\\omega) = \\sum_{j=1}^N  \\cos^2(\\omega t_j)$\n",
    "\n",
    "**NOTE I, R, M, S, C only depend on $\\omega$ and the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If N>>1 and we have data that extends longer than the period\n",
    "\n",
    "$S(\\omega) \\approx C(\\omega) \\approx N/2$ and $M(\\omega) \\ll N/2$ and\n",
    "\n",
    ">$Q \\approx V - {2\\over N} \\left[ a \\, I(\\omega) + b \\, R(\\omega)\\right]  + {1 \\over 2} (a^2 + b^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The posterior for many, randomly spaced, observations\n",
    "\n",
    "\n",
    "If we marginalize over $a$ and $b$ (as we are interested in the period)\n",
    "\n",
    "## $$  p(\\omega,\\sigma|\\{t,y\\}) \\propto  \\sigma^{-(N-2)} \\exp \\left( { - N V \\over 2 \\sigma^2} + { P(\\omega) \\over \\sigma^2}       \\right)$$\n",
    "\n",
    "with \n",
    "\n",
    "## $$P(\\omega) = {1 \\over N} [ I^2(\\omega) + R^2(\\omega)]$$\n",
    "\n",
    "## $$             V = {1\\over N} \\sum_{j=1}^N y_j^2$$\n",
    "\n",
    "## $$       I(\\omega) = \\sum_{j=1}^N y_j   \\sin(\\omega t_j)$$\n",
    "\n",
    "## $$ R(\\omega) = \\sum_{j=1}^N y_j  \\cos(\\omega t_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we know the noise $\\sigma$ then \n",
    "\n",
    "# $$   p(\\omega|\\{t,y\\}, \\sigma) \\propto \\exp \\left( { P(\\omega) \\over \\sigma^2} \\right)$$\n",
    "\n",
    "and we now have the posterior for $\\omega$! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$P(\\omega)$ is the [periodogram](https://en.wikipedia.org/wiki/Periodogram), which is just a plot of the \"power\" at each possible period\n",
    "\n",
    "So the process is similar to what we did with maximum likelihood estimation\n",
    "1. Get some data\n",
    "2. Make a uniform grid in period\n",
    "3. Evaluate the likelihood/the periodogram power at each period \n",
    "4. Identify the period with the maximum power\n",
    "\n",
    "<img src=\"periodogram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Significance of the peaks in the periodogram\n",
    "\n",
    "The amplitude(s) of the periodic signal can be derived from the posterior in much the same way as we do for MLE i.e. take the derivative of the posterior with respect to $a$ and $b$, equate to 0 and find where the maximum is.\n",
    "\n",
    "But what we really want to know is the \"best value\" $\\omega$? \n",
    "\n",
    "The $\\chi^2$ is given by\n",
    "$$\\chi^2(\\omega) \\equiv {1 \\over \\sigma^2} \\sum_{j=1}^N [y_j-y(t_j)]^2 =\n",
    "  {1 \\over \\sigma^2} \\sum_{j=1}^N [y_j- a_0\\, \\sin(\\omega t_j) - b_0 \\, \\cos(\\omega t_j)]^2$$\n",
    "  \n",
    "which we can simplify to\n",
    "\n",
    "$$\\chi^2(\\omega) =  \\chi_0^2 \\, \\left[1 - {2 \\over N \\, V}  \\, P(\\omega) \\right]$$\n",
    "\n",
    "where, again, $P(\\omega)$ is the periodogram and $\\chi_0^2$ is the $\\chi^2$ for a model with $y(t)$=constant:\n",
    "\n",
    "$$  \\chi_0^2 = {1 \\over \\sigma^2} \\sum_{j=1}^N y_j^2 = {N \\, V \\over \\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We'll now renormalise the periodogram, defining the [Lomb-Scargle periodogram](https://en.wikipedia.org/wiki/Least-squares_spectral_analysis#The_Lomb.E2.80.93Scargle_periodogram) as\n",
    "\n",
    "$$P_{\\rm LS}(\\omega) = \\frac{2}{N V} P(\\omega),$$  where $0 \\le P_{\\rm LS}(\\omega) \\le 1$.\n",
    "\n",
    "With this renormalization, the reduction in $\\chi^2(\\omega)$ for the harmonic model, \n",
    "relative to $\\chi^2$ for the pure noise model, $\\chi^2_0$ is\n",
    "$${\\chi^2(\\omega) \\over \\chi^2_0}=  1 - P_{LS}(\\omega).$$\n",
    "\n",
    "To determine if our source is variable or not, we first compute $P_{\\rm LS}(\\omega)$ and then model the odds ratio for our variability model vs. a no-variability model.\n",
    "\n",
    "If our variability model is \"correct\", then the peak of $P(\\omega)$ [found by grid search] gives the best $\\omega$ and the $\\chi^2$ at $\\omega = \\omega_0$ is $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If the true frequency is $\\omega_0$ then the maximum peak in the periodogram should have a height\n",
    "\n",
    "$$P(\\omega_0) = {N \\over 4} (a_0^2 + b_0^2)$$\n",
    "\n",
    "and standard deviation\n",
    "$$      \\sigma_P(\\omega_0)  = {\\sqrt{2} \\over 2} \\, \\sigma^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Properties of LS and the periodogram\n",
    "\n",
    "- The expected heights of the peaks in a periodogram don't depend on $\\sigma$ but their variation in height do.\n",
    "- For $P_{\\rm LS}(\\omega_0)$, with no noise the peak approaches 1. As noise increases, $P_{\\rm LS}(\\omega_0)$ decreases and is ``buried'' in the background  noise.\n",
    "- Our derivation worked for mean = 0, and data in a single channel/passband\n",
    "    - Extensions of Lomb-Scargle to multiple passbands and to handle a floating mean + many numerical approximations\n",
    "    \n",
    "## [Jake VanDerPlas has a good writeup on understanding the periodogram](https://arxiv.org/abs/1703.09824)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: Finding the Period of a Variable Star\n",
    "\n",
    "If you don't already have gatspy, you'll want it:\n",
    "\n",
    "> `conda install gatspy`\n",
    "\n",
    "There are alternatives but none are as good.\n",
    "\n",
    "There is some data in a file below.\n",
    "\n",
    "1. Plot it up to see what it looks like\n",
    "2. Use the Lomb-Scargle Multiband to construct a periodogram on a grid of a 1000 periods\n",
    "3. Plot the folded light curve for the best-guess period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "import astropy.table as at\n",
    "from gatspy.periodic import LombScargleMultiband\n",
    "\n",
    "data = at.Table.read('1938779.dat', format='ascii')\n",
    "\n",
    "t     = data['HJD']\n",
    "y     = data['MAG']\n",
    "dy    = data['MAGERR']\n",
    "filts = data['FILTS']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# PLOT THE DATA IN EACH PASSBAND - YOUR CODE HERE\n",
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# You can set the number of terms in the Lomb-Scargle periodogram, either baseline or by band\n",
    "model = LombScargleMultiband(Nterms_base=1, Nterms_band=0)\n",
    "model.fit(t, y, dy, filts)\n",
    "\n",
    "# CREATE A GRID OF PERIODS THAT IS REASONABLE - YOUR CODE HERE\n",
    "# CALL YOUR VARIABLE `periods`\n",
    "periods = np.linspace(____, ____, 1000)\n",
    "\n",
    "# YOU CAN COMPUTE THE PERIODOGRAM WITH:\n",
    "power = model.periodogram(periods)\n",
    "\n",
    "# PLOT THE PERIODOGRAM\n",
    "_____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# YOU CAN SET THE RANGE FOR THE PERIOD SEARCH TO NARROW THINGS DOWN\n",
    "model.optimizer.period_range = (_____, _____)\n",
    "\n",
    "# WITHIN THIS NARROW RANGE LOMB-SCARGLE CAN GIVE YOU THE BEST PERIOD\n",
    "period = model.best_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS TO LOOK AT YOUR FOLDED LIGHT CURVE\n",
    "\n",
    "tfit = np.linspace(0, period, 1000)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6))\n",
    "for pb in set(filts):\n",
    "    pfilt = [pb,]*len(tfit)\n",
    "    yfit = model.predict(tfit, filts=pfilt)\n",
    "    ax.plot(tfit, yfit, linestyle='-', marker='.', color='grey', alpha=0.1)\n",
    "    ind = (filts == pb)\n",
    "    ax.errorbar(t[ind]%period, y[ind], yerr=dy[ind], linestyle='None', marker='o')\n",
    "ax.set_xlabel('HJD')  \n",
    "ax.set_ylabel('MAG')\n",
    "fig.suptitle(f'Period = {period:.5f} days')\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What the periodogram is good for and what it isn't\n",
    "\n",
    "Pros:\n",
    "- Works with unevenly sampled data, and binning in phase helps build statistics even with noisy data\n",
    "\n",
    "Cons:\n",
    "- ANY PERIODIC SIGNAL IN THE DATA WILL EXHIBIT SOME POWER\n",
    "    - This includes aliases of the true period\n",
    "- Multiband extension requires  that the period/frequency is the same across all channels/passbands \n",
    "    - this is not the case for many time-series phenomena\n",
    "\n",
    "\n",
    "<img src=\"Sun-Wavelength-Chart.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analysis of Stochastic Processes\n",
    "\n",
    "If a system is always variable, but the variability is not (infinitely) predictable, then we have a [**stochastic**](https://en.wikipedia.org/wiki/Stochastic) process.  \n",
    "\n",
    "Stochastic does not mean you cannot characterize this process, and make statistical statements about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"quasar_schema.png\" style=\"float: left; width: 40%; margin-right: 1%;\"> <img src=\"ngc4261-large.jpg\" style=\"float: left; width: 50%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Take a (stochastically varying) quasar which has both *line* and *continuum* emission and where the line emission is stimulated by the continuum.  Since there is a physical separation between the regions that produce each type of emission, we get a delay between the light curves as can be seen here:\n",
    "\n",
    "![Peterson 2001, RM](https://ned.ipac.caltech.edu/level5/Sept01/Peterson2/Figures/figure24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The correlation function\n",
    "\n",
    "Instead of the auto-correlation function, we can look at the more general correlation function - this gives us information about the time delay between 2 processes.  \n",
    "\n",
    "If one time series is derived from another simply by shifting the time axis by $t_{\\rm lag}$, then their correlation function will have a peak at $\\Delta t = t_{\\rm lag}$.\n",
    "\n",
    "The correlation function between $f(t)$, and $g(t)$ is defined as\n",
    "# $${\\rm CF}(\\Delta t) = \\frac{\\lim_{T\\rightarrow \\infty}\\frac{1}{T}\\int_T f(t)g(t+\\Delta t)dt }{\\sigma_f \\sigma_g}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computing the correlation function is basically the mathematical processes of sliding the two curves over each other and computing the degree of similarity for each step in time.  \n",
    "\n",
    "The peak of the correlation function reveals the time delay between the processes.  Below we have the correlation function of the line and continuum emission from a quasar, which reveals a $\\sim$ 15 day delay between the two.\n",
    "\n",
    "![Peterson 2001, RM](https://ned.ipac.caltech.edu/level5/Sept01/Peterson2/Figures/figure25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What can the ACF tell us?\n",
    "\n",
    "If the values of $y$ are uncorrelated, then ACF$(\\Delta t)=0$ (except for ACF$(0)=1$).\n",
    "\n",
    "For processes that \"retain memory\" of previous states only for some characteristic time $\\tau$, the ACF will vanish for $\\Delta t \\gg \\tau$.\n",
    "\n",
    "Turning that around, the predictability of future behavior of future behavior of such a process is limited to times up to $\\sim \\tau$; you have to \"let the process run\" to know how it will behave at times longer than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Structure Function\n",
    "\n",
    "The *structure function* is another quantity that is frequently used in astronomy and is related to the ACF:\n",
    "\n",
    "# $${\\rm SF}(\\Delta t) = {\\sigma}_\\infty[1 - {\\rm ACF}(\\Delta t)]^{1/2}$$\n",
    "\n",
    "where ${\\sigma}_\\infty$ is the standard deviation of the time series as evaluated on timescales much larger than any charateristic timescale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The structure function is interesting because it's equal to the standard deviation of the distribution of the differences of $y(t_2) - y(t_1)$ evaluated at many different $t_1$ and $t_2$ (i.e., with a time lag of $\\Delta t = t_2 - t_1$), and divided by $\\sqrt 2$.\n",
    "\n",
    "This is of practical use: if I have a series of observations $y_i$ (taken at random times $t_i$) it's relatively straighforward to compute the structure function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Damped Random Walk\n",
    "\n",
    "A DRW is described by a stochastic differential equation which includes a damping term that pushes $y(t)$ back towards the mean, hence the name **damped random walk**.   \n",
    "\n",
    "The ACF for a DRW is given by\n",
    "\n",
    "# $$ ACF(t) = \\exp(-t/\\tau)$$\n",
    "where $\\tau$ is the characteristic timescale (i.e., the damping timescale).\n",
    "\n",
    "The DRW structure function can be written as\n",
    "# $$ SF(t) = \\sigma_{\\infty}[1-\\exp(-t/\\tau)]^{1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Notice what's happening\n",
    "\n",
    "We are **not** writing down a model for the observations directly anymore (they are a stochastic process so what'd be be the point)\n",
    "\n",
    "**We are writing down a model for how the observations are correlated with each other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Structure Function for Dampled Random Walk\n",
    "\n",
    "<img src=\"MacLeod2010.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SF example above was an example of a DRW: the light curve is strongly correlated a short timescales, but uncorrelated at long timescales. \n",
    "\n",
    "This is observed in optical variability of quasar continuum light; in fact, it works so well that one can use this model to distinguish quasars from stars, based solely on the variability they exhibit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you can make the ACF or SF, then you can jolly well take it's Fourier Transform to get the:\n",
    "\n",
    "### Power Spectral Density\n",
    "\n",
    "The Fourier Transform of an ACF is the [Power Spectral Density (PSD)](https://en.wikipedia.org/wiki/Spectral_density).  So, the PSD is an analysis in frequency space and the ACF is in time space.\n",
    "\n",
    "(the Wiener-Khinchin theorem describes the fact that the ACF and PSD are a Fourier pair) \n",
    "\n",
    "For example, for a sinusoidal function in time space, the ACF will have the same period, $T$. Conversly, the PSD in frequency space will be a $\\delta$ function centered on $\\omega = 1/2\\pi T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our nice quasar without an analytic model but with an analytic form for the ACF, the PSD is then:\n",
    "\n",
    "# $$ PSD(f) = \\frac{\\tau^2 SF_{\\infty}^2}{1+(2\\pi f \\tau)^2}$$\n",
    "\n",
    "which means that a DRW is a $1/f^2$ process at high frequency. The **damped** part comes from the flat PSD at low frequency.\n",
    "\n",
    "More generically, if \n",
    "\n",
    "## $${\\rm SF} \\propto t^{\\alpha}$$\n",
    "\n",
    "then \n",
    "\n",
    "## $${\\rm PSD} \\propto \\frac{1}{f^{1+2\\alpha}}$$\n",
    "\n",
    "So an analysis of a stochastic system can be done with either the ACF, SF, or PSD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different stochastic processes can be categorized based on their ACF/PSD\n",
    "\n",
    "* A stochastic process with $1/f^2$ spectrum is known as random walk (if discrete) or Brownian motion (or, more accurately, Wiener process) if continuous. These physically occur when the value being observed is subjected to a series of independent changes of similar size. It's also sometimes called as \"red noise\". Quasar variability exhibits $1/f^2$ properties at high frequencies (that is, short time scales, below a year or so). \n",
    "\n",
    "* A stochastic process with $1/f$ spectrum are sometimes called \"long-term memory processes\" (also sometimes know as \"pink noise\"). They have equal energy at all octaves (or over any other logarithmic frequency interval). This type of process has infinite variance and an undefined mean (similar to a Lorentzian distribution). \n",
    "\n",
    "* A process with a constant PSD is frequently referred to as \"white noise\" -- it has equal intensity at all frequencies. This is a process with no memory -- each measurement is independent of all others. i.e. white noise is I.I.D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: Stochastic Processes and the ACF\n",
    "\n",
    "AstroML has [time series](http://www.astroml.org/modules/classes.html#module-astroML.time_series) and [Fourier](http://www.astroml.org/modules/classes.html#module-astroML.fourier) tools for generating light curves drawn from a power law in frequency space.  \n",
    "\n",
    "Note that these tools define $\\beta = 1+2\\alpha$  ($\\beta=2$ for a random walk). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.time_series import generate_power_law\n",
    "from astroML.fourier import PSD_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 2014\n",
    "dt = 0.01\n",
    "\n",
    "\n",
    "betaRed = ___  # Complete\n",
    "betaPink = ___  # Complete\n",
    "betaWhite = ___  # Complete\n",
    "\n",
    "t = dt * np.arange(N)\n",
    "yRed = generate_power_law(N, dt, betaRed)\n",
    "yPink = generate_power_law(N, dt, _____) # Complete\n",
    "yWhite = generate_power_law(N, dt, ______)/10.0 # Complete\n",
    "\n",
    "\n",
    "\n",
    "fRed, PSDred = PSD_continuous(t, _____)   # Complete\n",
    "fPink, _____ = PSD_continuous(t, _____)       # Complete\n",
    "fWhite, _____ = PSD_continuous(t, yWhite)    # Complete\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(t, yWhite, c='Grey')\n",
    "ax1.plot(t, yPink, c='Pink')\n",
    "ax1.plot(t, yRed, '-r')\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_title('Real Space')\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('fake mag')\n",
    "\n",
    "ax2 = fig.add_subplot(122, xscale='log', yscale='log')\n",
    "ax2.plot(fWhite, ______, c='Grey')   \n",
    "ax2.plot(fPink, ______, c='Pink')  \n",
    "ax2.plot(fRed, PSDred, '-r')  \n",
    "ax2.set_xlim(1E-1, 60)\n",
    "ax2.set_ylim(1E-11, 1E-3)\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_xlabel('Power')\n",
    "ax2.set_title('PSD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ACF for Unevenly Sampled Data\n",
    "\n",
    "astroML also has tools for computing the ACF of unevenly sampled data using two different (Scargle) and (Edelson & Krolik) methods: [http://www.astroml.org/modules/classes.html#module-astroML.time_series](http://www.astroml.org/modules/classes.html#module-astroML.time_series)\n",
    "\n",
    "One of the tools is for generating a **damped random walk (DRW)**.  Above we found that a random walk had a $1/f^2$ PSD.  A *damped* random walk is a process \"remembers\" its history only for a characteristic time, $\\tau$. The ACF vanishes for $\\Delta t \\gg \\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstroMLDeprecationWarning: The lomb_scargle function is deprecated and may be removed in a future version.\n",
      "        Use astropy.stats.LombScargle instead. [astroML.time_series.ACF]\n",
      "WARNING: AstroMLDeprecationWarning: The lomb_scargle function is deprecated and may be removed in a future version.\n",
      "        Use astropy.stats.LombScargle instead. [astroML.time_series.ACF]\n"
     ]
    }
   ],
   "source": [
    "# Syntax for EK and Scargle ACF computation\n",
    "import numpy as np\n",
    "from astroML.time_series import generate_damped_RW\n",
    "from astroML.time_series import ACF_scargle, ACF_EK\n",
    "\n",
    "t = np.arange(0,1000)\n",
    "y = generate_damped_RW(t, tau=300)\n",
    "dy = 0.1\n",
    "y = np.random.normal(y,dy)\n",
    "\n",
    "ACF_scargle, bins_scargle = ACF_scargle(t,y,dy)\n",
    "ACF_EK, ACF_err_EK, bins_EK = ACF_EK(t,y,dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ivezic, Figure 10.30\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.time_series import lomb_scargle, generate_damped_RW\n",
    "from astroML.time_series import ACF_scargle, ACF_EK\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate time-series data:\n",
    "#  we'll do 1000 days worth of magnitudes\n",
    "\n",
    "t = np.arange(0, 1E3)\n",
    "z = 2.0\n",
    "tau = 300\n",
    "tau_obs = tau / (1. + z)\n",
    "\n",
    "np.random.seed(6)\n",
    "y = generate_damped_RW(t, tau=tau, z=z, xmean=20)\n",
    "\n",
    "# randomly sample 100 of these\n",
    "ind = np.arange(len(t))\n",
    "np.random.shuffle(ind)\n",
    "ind = ind[:100]\n",
    "ind.sort()\n",
    "t = t[ind]\n",
    "y = y[ind]\n",
    "\n",
    "# add errors\n",
    "dy = 0.1\n",
    "y_obs = np.random.normal(y, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute ACF via scargle method\n",
    "C_S, t_S = ACF_scargle(t, y_obs, dy, n_omega=2 ** 12, omega_max=np.pi / 5.0)\n",
    "\n",
    "ind = (t_S >= 0) & (t_S <= 500)\n",
    "t_S = t_S[ind]\n",
    "C_S = C_S[ind]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute ACF via E-K method\n",
    "C_EK, C_EK_err, bins = ACF_EK(t, y_obs, dy, bins=np.linspace(0, 500, 51))\n",
    "t_EK = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plot the input data\n",
    "ax = fig.add_subplot(211)\n",
    "ax.errorbar(t, y_obs, dy, fmt='.k', lw=1)\n",
    "ax.set_xlabel('t (days)')\n",
    "ax.set_ylabel('observed flux')\n",
    "\n",
    "# plot the ACF\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(t_S, C_S, '-', c='gray', lw=1, label='Scargle')\n",
    "ax.errorbar(t_EK, C_EK, C_EK_err, fmt='.k', lw=1, label='Edelson-Krolik')\n",
    "ax.plot(t_S, np.exp(-abs(t_S) / tau_obs), '-k', label='True')\n",
    "ax.legend(loc=3)\n",
    "\n",
    "ax.plot(t_S, 0 * t_S, ':', lw=1, c='gray')\n",
    "\n",
    "ax.set_xlim(0, 500)\n",
    "ax.set_ylim(-1.0, 1.1)\n",
    "\n",
    "ax.set_xlabel('t (days)')\n",
    "ax.set_ylabel('ACF(t)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next week\n",
    "\n",
    "* (Continious) Autoregressive and Moving Average models \n",
    "* Gaussian Processes"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
