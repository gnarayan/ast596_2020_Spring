{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "We've already looked at the simple, most general Monte Carlo algorithm, and you used it on your last homework set.\n",
    "\n",
    "## Simple Monte Carlo\n",
    "\n",
    "A posterior is already naturally factored into a likelihood function and a prior PDF.\n",
    "\n",
    "## $$p(\\theta|x) \\propto p(x|\\theta)\\,p(\\theta)$$\n",
    "\n",
    "Applying this in the MC integration context leads to the Simple Monte Carlo algorithm:\n",
    "\n",
    "```\n",
    "while we want more samples\n",
    "    draw theta from p(theta)\n",
    "    compute weight = p(x|theta)\n",
    "    store theta and weight\n",
    "```\n",
    "\n",
    "Obtaining marginal distribution(s) for $\\theta$ then reduces to constructing weighted histograms of the samples.\n",
    "\n",
    "SMC is indeed simple (as long as the prior is simple to draw from), but if the priors are not very informative then it still wastes many likelihood evaluations where the posterior is small. \n",
    "\n",
    "However, refinements of this approach lead to some more advanced algorithms. Once class of refinements is in how the samples are drawn:\n",
    "\n",
    "### Rejection sampling\n",
    "For this method, we need to define an *envelope function* which everywhere exceeds the target PDF, $p(x)$, and can be sampled. Let this be $Ag(x)$ where $A$ is a scaling factor and $g(x)$ is a PDF we know.\n",
    "\n",
    "Then the algorithm is\n",
    "```\n",
    "while we want more samples\n",
    "    draw x from g(x)\n",
    "    draw u from Uniform(0,1)\n",
    "    if u <= p(x)/(A*g(x)), keep the sample x\n",
    "    otherwise, reject x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Rejection Sampling\n",
    "\n",
    "Implement a rejection sampler corresponding to the example figure above that illustrates the method. For this example,\n",
    "\n",
    "* $p(x)$ is the $\\chi^2$ distribution with 3 degrees of freedom\n",
    "\n",
    "* $A=\\pi$\n",
    "\n",
    "* $g(x)$ is a normal distribution with mean 0 and standard deviation 5\n",
    "\n",
    "Verify that your samples do indeed approximate the target PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is at the heart of a  very widely used (by scientists anyway) and simple to implement technique: the **Metropolis-Hastings** algorithm, which is one example of *Markov Chain Monte Carlo.*\n",
    "\n",
    "### Markov Chains\n",
    "\n",
    "A Markov Chain is a sequence where the $n$th entry depends explicitly on the $(n-1)$th, but not (explicitly) on previous steps. The chain will be a random walk through parameter space.\n",
    "\n",
    "\n",
    "### Formalities of MCMC\n",
    "\n",
    "Markov chains provide a powerful way to sample PDFs, provided that the transition kernel/proposal distribution - how we go from state 1 to state 2  satisfies a few requirements\n",
    "* Detailed balance: any transition must be reversible; the probability of being at $x$ and moving to $y$ must equal the probability of being at $y$ and moving to $x$\n",
    "* Ergodicity: the process may not be periodic, but it nevertheless must be possible to return to a given state in finite time\n",
    "* It must be possible, in principle, to reach any state with non-zero prior probability\n",
    "\n",
    "#### Why does it work?\n",
    "The probability of an arbitrary point from such a chain being located at $x'$ is (marginalizing over the possible immediately preceding points)\n",
    "\n",
    "## $$p(y) = \\int dx \\, p(x) \\, T(x'|x)$$\n",
    "\n",
    "where $T(y|x)$ is the transition probability of a step from $x$ to $x'$.\n",
    "\n",
    "If we have detailed balance, \n",
    "\n",
    "## $$p(x)T(x'|x) = p(x')T(x|x')$$\n",
    "\n",
    "rearranging:\n",
    "\n",
    "## $$ \\frac{T(x'|x)}{T(x|x')} = \\frac{p(x')}{p(x)} $$\n",
    "\n",
    "The basic trick to connect this with rejection sampling is to break the transition into two steps:\n",
    "1. A proposal, g(x'| x)\n",
    "and \n",
    "2. Acceptance ratio, A(x'|x)\n",
    "\n",
    "i.e. \n",
    "\n",
    "## $$ T(x'|x) = A(x'|x) g(x'| x) $$ \n",
    "\n",
    "rearranging again :\n",
    "\n",
    "## $$ \\frac{A(x'|x)}{A(x|x')} = \\frac{p(x')g(x|x')}{p(x)g(x'|x) }$$\n",
    "\n",
    "\n",
    "## Metropolis-Hastings\n",
    "This algorithm can be thought of as an MCMC adaptation of rejection sampling. We need to define\n",
    "1. An initial state (parameter values)\n",
    "2. A proposal distribution, $g(x'|x)$, giving the probability that we attempt to move from $x$ to $x'$.\n",
    "\n",
    "Let $P$ be the distribution we want to sample. The algorithm is then\n",
    "```\n",
    "set x to an initial state\n",
    "compute p(x)\n",
    "while we want more samples\n",
    "    draw y from g(x'|x)\n",
    "    compute p(x')\n",
    "    draw u from Uniform(0,1)\n",
    "    if u <= p(x')/p(x) * g(x|x')/g(x'|x), set the state x=x'\n",
    "    otherwise, x stays the same\n",
    "    store x as a sample\n",
    "```\n",
    "\n",
    "Compare this to the rejection sampling algorithm above!\n",
    "\n",
    "Notice that the probability of accepting a step  (once it's proposed) is\n",
    "\n",
    "## $$A(x',x) = \\mathrm{min}\\left[1, \\frac{p(x')g(x|x')}{p(x)g(x'|x)}\\right]$$\n",
    "\n",
    "Let's look again at the requirement of detailed balance\n",
    "\n",
    "> the probability of being at $x$ and moving to $y$ must equal the probability of being at $y$ and moving to $x$\n",
    "\n",
    "The first of these is $p(x)g(x'|x)A(x',x)$, where\n",
    "\n",
    "* $p(x)$ is the posterior density (probability of *being* at $x$, if we're sampling $P$ properly)\n",
    "\n",
    "* $g(x'|x)$ is the proposal distribution (probability of attempting a move to $y$ from $x$)\n",
    "\n",
    "* $A(x',x)$ is the probability of accepting the proposed move\n",
    "\n",
    "With this definition of $A$, detailed balance is automatically satisfied!\n",
    "\n",
    "## $$p(x)g(x'|x)A(x',x) \\equiv p(x')g(x|x')A(x,x')$$\n",
    "\n",
    "Note that **even if a step is rejected, we still keep a sample** (the original state, without moving). The difficulty of finding a temptingly better point is important information!\n",
    "\n",
    "\n",
    "### Metropolis\n",
    "If the proposal distribution is translation invariant (i.e. only depends on the distance between the points), $g(x'|x)=g\\left(\\left|x'-x\\right|\\right)$, then it drops out of the *acceptance ratio* that decides whether to accept a step. \n",
    "\n",
    "**The most basic choice you can make is a Gaussian.**\n",
    "\n",
    "For an N-dimensional Gaussian proposal function, $g$,  an *acceptance fraction* $A$ of $\\sim25\\%$ is optimal.\n",
    "\n",
    "## $$A(x',x) = \\mathrm{min}\\left[1, \\frac{p(y)}{p(x)}\\right]$$\n",
    "\n",
    "This is the original Metropolis algorithm, and is the easiest case to implement.\n",
    "\n",
    "In this case, we *always* accept a jump to higher $p$, and *sometimes* accept one to lower $p$.\n",
    "\n",
    "You'll have to tune the $\\sigma$ of your Gaussian proposal function g by hand to make sure that your get a ~25% acceptance ratio. Note that this sigma is simply telling you the distribution of $x'$ from $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Metropolis\n",
    "\n",
    "\n",
    "You guessed it... Implement the Metroplis Hastings algorithm in python.\n",
    "Your implementation should accept an arbitary function as an argument (just as you've been passing to `scipy.optimize`)\n",
    "\n",
    "Test it by sampling both the `circle` and `pgauss` function below 10,000 times and plotting your samples as you did in class (don't plot up your rejected). You can implement a prior function that imposes bounds. \n",
    "\n",
    "Look at your first 100 samples and then samples 500 to 600 for the `pgauss` case and comment on the difference. Think about this on Problem 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "def circle(x, y):\n",
    "    return (x-1)**2 + (y-2)**2 - 3**2\n",
    "\n",
    "mus = np.array([5, 5])\n",
    "sigmas = np.array([[1, .9], [.9, 1]])\n",
    "\n",
    "def pgauss(x, y):\n",
    "    return st.multivariate_normal.pdf([x, y], mean=mus, cov=sigmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Using your sampler for a real problem where a grid would have been really painful.\n",
    "\n",
    "\n",
    "We'll be analyzing data from the Optical Gravitational Lensing Experiment (OGLE), which monitors stars in our galaxy in the hopes of detecting gravitational microlensing events that occur when a compact mass (e.g. a fainter star) passes in front of the monitored star.\n",
    "\n",
    "You can read more about microlensing here if you like:\n",
    "https://en.wikipedia.org/wiki/Gravitational_microlensing\n",
    "\n",
    "Data are available through the [OGLE Early Warning System](http://ogle.astrouw.edu.pl/ogle4/ews/ews.html). Scroll down a bit to the list of recent events and choose one to analyze. (Not the one shown below. Be original.) The event summary page will include a plot like this.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"http://ogle.astrouw.edu.pl/ogle4/ews/2019/data/2019/blg-0001/lcurve.gif\" width=75%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "As long as a vaguely reasonable looking magenta line is shown, this should be a good data set to fit. Download the `phot.dat` for your chosen event (linked at the bottom of the webpage).\n",
    "\n",
    "As described on the OGLE page, the columns of this text file are\n",
    "\n",
    "> Hel.JD, I magnitude, magnitude error, seeing estimation (in pixels - 0.26\"/pixel) and sky level\n",
    "\n",
    "* Heliocentric Julian Date. This is time, measured in days, since a fixed reference. The \"heliocentric\" part means that it has been corrected to the reference frame of the Sun, i.e. the few minutes of light travel time more or less that would affect photon arrivals at different parts of the Earth's year have been subtracted off.\n",
    "\n",
    "* Measurements of magnitude in the $I$ band (a near infrared band). Recall that astronomical magnitude, relative to a given reference source, is given by the relationship $m = m_\\mathrm{ref} - 2.5\\,\\log_{10}\\left(\\frac{F}{F_\\mathrm{ref}}\\right)$, where $F$ is flux.\n",
    "\n",
    "* Measurement uncertainty on the $I$ magnitude, defined in some unspecified way (digging through papers might elucidate this).\n",
    "\n",
    "* The \"seeing\" and \"sky level\" quantities refer to the observing conditions, which we will not work with directly. These will have been accounted for (somehow) in deriving the best-fitting magnitude and its uncertainty.\n",
    "\n",
    "\n",
    "As Bayesian's we have 4 questions to answer\n",
    "1. What's the model?\n",
    "2. What's the Likelihood?\n",
    "3. What's the Prior?\n",
    "4. How do you sample?\n",
    "\n",
    "\n",
    "I'll tell you 1. and you've already written down the answer to 4.\n",
    "\n",
    "\n",
    "## $$F(t) = F_0 \\frac{u(t)^2 + 2}{y(t)\\sqrt{u(t)^2+4}}$$\n",
    "\n",
    "where\n",
    "\n",
    "## $$u(t) = \\sqrt{p^2 + \\left( \\frac{t-t_\\mathrm{max}}{t_\\mathrm{E}} \\right)^2}$$\n",
    "\n",
    "You'll of course also need the transformation between flux and magnitude, above. For convenience, let's parameterize the normalization of the model lightcurve in magnitudes rather than flux, i.e. $I_0$ rather than $F_0$; that way, all of the \"ref\" quantities in the magnitude definition are absorbed into this new parameter and we won't have to worry about them explicitly. With that substitution, the model parameters are $I_0$, $p$, $t_\\mathrm{max}$ and $t_\\mathrm{E}$. \n",
    "\n",
    "* $t_\\mathrm{E}$ is the Einstein crossing time\n",
    "\n",
    "* $t_\\mathrm{max}$ is the time of maximum - which you should be able to read off from the plot\n",
    "\n",
    "* $p$ is the peak magnification \n",
    "\n",
    "\n",
    "Lacking any better information, we'll assume that the sampling distributions for the magnitude measurements are Gaussian and independent, with means given by the \"magnitude\" column and standard deviations given by the \"magnitude error\" column, and that the time stamps are exact.\n",
    "\n",
    "\n",
    "Do an MCMC fit of this microlensing model to your lightcurve data\n",
    "This fit should be doable, if potentially annoying, with your Metropolis-Hastings implementation.\n",
    "\n",
    "Your solution should include the following:\n",
    "\n",
    "1. expressions (in readable code, at a minimum) of the prior distributions and likelihood encoded (i.e. Q2 for a Bayesian)\n",
    "2. some justification of the choice of priors (Q3. for a Bayesian can be brief)\n",
    "3. plots showing traces of each parameter, and an identified burn-in period\n",
    "4. some evaluation of how well the fit has converged (see below)\n",
    "5. 1D histograms of the parameter samples and 2D contour plots of parameter pairs (you can just use the `corner` package for this, which makes it trivial - you can see how in homework from week 1)\n",
    "6. \"best fit\" values and 68.3% confidence intervals from the 1D marginalized posteriors of each parameter. \n",
    "7. a plot of the best-fitting model lightcurve over the data, and some qualitative comments about how good a fit it appears (hint: depending on your data set, you may need to zoom in quite a lot to get a good look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
